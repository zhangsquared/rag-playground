{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f914fc53",
   "metadata": {},
   "source": [
    "## Step 1: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce17d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "[0] Cloud Native Computing Foundation (“CNCF”) Charter\n",
      "\n",
      "[1] The Linux Foundation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def split_into_chunks(doc_file: str) -> List[str]:\n",
    "    with open(doc_file, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return [chunk for chunk in content.split(\"\\n\\n\")]\n",
    "\n",
    "chunks = split_into_chunks(\"doc.md\")\n",
    "\n",
    "# print out the first two chunks\n",
    "print(len(chunks))\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498ed33",
   "metadata": {},
   "source": [
    "## Step 2: Embedding into vector\n",
    "\n",
    "(The initial run may be slow because the model needs to be downloaded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8183e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsquared/projects/rag-playground/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.06328241527080536, 0.05413154140114784]\n",
      "Embedded 222 chunks.\n",
      "First embedding length: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    embedding = embedding_model.encode(chunk, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "\n",
    "# Test embedding function\n",
    "embedding = embed_chunk(\"test-content\")\n",
    "print(len(embedding))\n",
    "print(embedding[:2]) # print first two dimensions of the embedding\n",
    "\n",
    "# Actually embed all chunks\n",
    "embeddings = [embed_chunk(chunk) for chunk in chunks]\n",
    "print(f\"Embedded {len(embeddings)} chunks.\")\n",
    "print(f\"First embedding length: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e30c46",
   "metadata": {},
   "source": [
    "## Step 3: Save the result into a vector DB\n",
    "\n",
    "Requires: the original chuck (string), the embeding (float), id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c478dd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 222 chunks and embeddings to ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chromadb_client = chromadb.EphemeralClient(\n",
    "    Settings(allow_reset=True)\n",
    ")\n",
    "chromadb_collection = chromadb_client.get_or_create_collection(name=\"default\")\n",
    "\n",
    "def save_embeddings(chunks: List[str], embeddings: List[List[float]]) -> None:\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        chromadb_collection.add(\n",
    "            documents=[chunk],\n",
    "            embeddings=[embedding],\n",
    "            ids=[str(i)]\n",
    "        )\n",
    "    print(f\"Saved {len(chunks)} chunks and embeddings to ChromaDB.\")\n",
    "\n",
    "save_embeddings(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cef579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromadb_client.reset()  # Use with caution: this will delete all data in the ChromaDB instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f985ff",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve relevant chunks based on a query\n",
    "\n",
    "Embed the query as well\n",
    "Query from Chroma that has the top 5 closes vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29cf337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] -\t(a) The CNCF Governing Board will be responsible for marketing and other business oversight and budget decisions for the CNCF. The Governing Board does not make technical decisions for the CNCF, other than working with the TOC to set the overall scope for the CNCF as described in the cloud native definition from Section 1.\n",
      "\n",
      "[1] \t-\ti. Appoint one (1) representative to the CNCF Governing Board.\n",
      "\n",
      "[2] #### 2. Role of the CNCF.\n",
      "\n",
      "[3] The CNCF will serve a role in the open source community responsible for:\n",
      "\n",
      "[4] \t\t-\td. the CNCF Executive Director, or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query: str, top_k: int) -> List[str]:\n",
    "    query_embedding = embed_chunk(query)\n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results['documents'][0]\n",
    "\n",
    "query = \"What is the responsibility of CNCF Governing Board?\"\n",
    "retrieved_chunks = retrieve(query, 5)\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b668f",
   "metadata": {},
   "source": [
    "## Step 5: Re-rank\n",
    "\n",
    "The results retrieved from the vector database are fast but lack accuracy. \n",
    "We can use a CrossEncoder for re-ranking, which is slower but more accurate. \n",
    "(The initial run may be slow because the model needs to be downloaded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28cc0f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] -\t(a) The CNCF Governing Board will be responsible for marketing and other business oversight and budget decisions for the CNCF. The Governing Board does not make technical decisions for the CNCF, other than working with the TOC to set the overall scope for the CNCF as described in the cloud native definition from Section 1.\n",
      "\n",
      "[1] The CNCF will serve a role in the open source community responsible for:\n",
      "\n",
      "[2] #### 2. Role of the CNCF.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:\n",
    "    cross_encoder = CrossEncoder('BAAI/bge-reranker-base')\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    scored_chunks = list(zip(retrieved_chunks, scores))\n",
    "    scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [chunk for chunk, _ in scored_chunks][:top_k]\n",
    "\n",
    "reranked_chunks = rerank(query, retrieved_chunks, 3)\n",
    "\n",
    "for i, chunk in enumerate(reranked_chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429c3bd",
   "metadata": {},
   "source": [
    "## Step 6: Send the related chucks and user query to a LLM\n",
    "\n",
    "Here we choose `gemini-2.5-flash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014706b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a knowledge assistant. Please generate an accurate answer based on the user's question and the following passages.\n",
      "\n",
      "User question: What is the responsibility of CNCF Governing Board?\n",
      "\n",
      "Relevant context:\n",
      "-\t(a) The CNCF Governing Board will be responsible for marketing and other business oversight and budget decisions for the CNCF. The Governing Board does not make technical decisions for the CNCF, other than working with the TOC to set the overall scope for the CNCF as described in the cloud native definition from Section 1.\n",
      "\n",
      "The CNCF will serve a role in the open source community responsible for:\n",
      "\n",
      "#### 2. Role of the CNCF.\n",
      "\n",
      "Please answer based only on the content above and do not fabricate information.\n",
      "\n",
      "---\n",
      "\n",
      "Final Answer:\n",
      "The CNCF Governing Board is responsible for marketing and other business oversight and budget decisions for the CNCF. It does not make technical decisions for the CNCF, other than working with the TOC to set the overall scope for the CNCF as described in the cloud native definition from Section 1.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "google_client = genai.Client(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "def generate(query: str, chunks: List[str]) -> str:\n",
    "    prompt = f\"\"\"You are a knowledge assistant. Please generate an accurate answer based on the user's question and the following passages.\n",
    "\n",
    "User question: {query}\n",
    "\n",
    "Relevant context:\n",
    "{\"\\n\\n\".join(chunks)}\n",
    "\n",
    "Please answer based only on the content above and do not fabricate information.\"\"\"\n",
    "\n",
    "    print(f\"{prompt}\\n\\n---\\n\")\n",
    "\n",
    "    response = google_client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "answer = generate(query, reranked_chunks)\n",
    "print(\"Final Answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
